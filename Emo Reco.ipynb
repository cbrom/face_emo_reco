{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotions = [\"neutral\", \"anger\", \"disgust\", \"happy\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = \"ck_export/dataset/train\"\n",
    "test_data_dir = \"ck_export/dataset/test\"\n",
    "def load_dataset(dataset_dir):\n",
    "    images = []\n",
    "    image_labels = []\n",
    "    for index, emotion in enumerate(emotions):\n",
    "        for image in glob.glob(\"%s/%s/*\"% (dataset_dir, emotion)):\n",
    "            image = cv2.imread(image)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            images.append(gray)\n",
    "            image_labels.append(index)\n",
    "    return images, image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fishface = cv2.face.FisherFaceRecognizer_create()\n",
    "\n",
    "def train_test_set(dataset_path):\n",
    "    training_data, training_labels = load_dataset(dataset_path + \"train\")\n",
    "    test_data, test_labels = load_dataset(dataset_path + \"test\")\n",
    "    \n",
    "    return np.array(training_data), training_labels, np.array(test_data), test_labels\n",
    "\n",
    "\n",
    "def run_recognizer():\n",
    "    training_data, training_labels, test_data, test_labels = train_test_set(\"./ck_export/dataset/\")\n",
    "    print(\"training fisher face classifier\")\n",
    "    print(\"size of training set is:\", len(training_labels), \"images\")\n",
    "    fishface.train(training_data, np.asarray(training_labels))\n",
    "    \n",
    "    print(\"predicting classification set\")\n",
    "    cnt=0\n",
    "    correct=0\n",
    "    incorrect=0\n",
    "    for image in test_data:\n",
    "        pred, conf = fishface.predict(image)\n",
    "        if pred == test_labels[cnt]:\n",
    "            correct += 1\n",
    "            cnt += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            cnt += 1\n",
    "    return ((100*correct)/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_labels = load_dataset(test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217, 350, 350)\n"
     ]
    }
   ],
   "source": [
    "l = np.array(test_data)\n",
    "print(l.shape)\n",
    "# pred, conf = fishface.predict(l[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training fisher face classifier\n",
      "size of training set is: 874 images\n",
      "predicting classification set\n",
      "got 98.15668202764977 percent correct! \n"
     ]
    }
   ],
   "source": [
    "correct = run_recognizer()\n",
    "fishface.save(\"fisher face classifer.xml\")\n",
    "print(\"got\", correct, \"percent correct! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faceDet = cv2.CascadeClassifier(\"OpenCV_FaceCascade/haarcascade_frontalface_default.xml\")\n",
    "faceDet_two = cv2.CascadeClassifier(\"OpenCV_FaceCascade/haarcascade_frontalface_alt2.xml\")\n",
    "faceDet_three = cv2.CascadeClassifier(\"OpenCV_FaceCascade/haarcascade_frontalface_alt.xml\")\n",
    "faceDet_four = cv2.CascadeClassifier(\"OpenCV_FaceCascade/haarcascade_frontalface_alt_tree.xml\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "feelings_faces = []\n",
    "for index, emotion in enumerate(emotions):\n",
    "  feelings_faces.append(cv2.imread('./emojis/' + emotion + '.png', -1))\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    if frame is not None:\n",
    "        # Predict result with network\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "#         gray = gray + 40\n",
    "        \n",
    "        #Detect face using 4 different classifiers\n",
    "        face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_two = faceDet_two.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_three = faceDet_three.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_four = faceDet_four.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "        #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "        if len(face) == 1:\n",
    "            facefeatures = face\n",
    "        elif len(face_two) == 1:\n",
    "            facefeatures = face_two\n",
    "        elif len(face_three) == 1:\n",
    "            facefeatures = face_three\n",
    "        elif len(face_four) == 1:\n",
    "            facefeatures = face_four\n",
    "        else:\n",
    "            facefeatures = \"\"\n",
    "        \n",
    "        #Cut face\n",
    "        \n",
    "        for i, (x, y, w, h) in enumerate(facefeatures): #get coordinates and size of rectangle containing face\n",
    "#             print(\"face found in file: %s\" %f)\n",
    "            gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "            gray = cv2.resize(gray, (350, 350)) #Resize face so all images have same size\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        \n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         gray = cv2.resize(gray, (350, 350))\n",
    "            pred, conf = fishface.predict(gray)\n",
    "        \n",
    "            for index, emotion in enumerate(emotions):\n",
    "                cv2.putText(frame, emotion, (10, index * 20 + 20), cv2.FONT_HERSHEY_PLAIN, 0.5, (0, 255, 0), 1);\n",
    "#             cv2.rectangle(frame, (130, index * 20 + 10), (130 + int(pred * 100), (index + 1) * 20 + 4), (255, 0, 0), -1)\n",
    "        \n",
    "            face_image = feelings_faces[pred]\n",
    "\n",
    "        # Ugly transparent fix\n",
    "            for c in range(0, 3):\n",
    "                frame[200 + (i*50):320, 10:130, c] = face_image[:,:,c] * (face_image[:, :, 3] / 255.0) +  frame[200:320, 10:130, c] * (1.0 - face_image[:, :, 3] / 255.0)\n",
    "\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def brightness_script(dataset_dir):\n",
    "    for index, emotion in enumerate(emotions):\n",
    "        for image in glob.glob(\"%s/%s/*\"% (dataset_dir, emotion)):\n",
    "            image = cv2.imread(image)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            for i in range(10, 100)\n",
    "    return images, image_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
